{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCNN_256_256_x00\n",
      "Number of patches per image:  48\n",
      "Batch size:  10\n",
      "Evaluate batch size:  10\n",
      "Channels last\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_172 (TimeDi (None, 48, 254, 254, 16)  448       \n",
      "_________________________________________________________________\n",
      "time_distributed_173 (TimeDi (None, 48, 84, 84, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_174 (TimeDi (None, 48, 82, 82, 32)    4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_175 (TimeDi (None, 48, 41, 41, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_176 (TimeDi (None, 48, 41, 41, 64)    18496     \n",
      "_________________________________________________________________\n",
      "time_distributed_177 (TimeDi (None, 48, 20, 20, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_178 (TimeDi (None, 48, 20, 20, 64)    36928     \n",
      "_________________________________________________________________\n",
      "time_distributed_179 (TimeDi (None, 48, 6, 6, 64)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_180 (TimeDi (None, 48, 4, 4, 32)      18464     \n",
      "_________________________________________________________________\n",
      "time_distributed_181 (TimeDi (None, 48, 1, 1, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_182 (TimeDi (None, 48, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 153,860\n",
      "Trainable params: 153,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 300 images belonging to 4 classes.\n",
      "Found 100 images belonging to 4 classes.\n",
      "Epoch 001\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[480,254,254,16]\n\t [[Node: time_distributed_172/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_172/Reshape, time_distributed_172/kernel/read)]]\n\t [[Node: metrics_16/acc/Mean/_3271 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2576_metrics_16/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'time_distributed_172/convolution', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-6d56bcdf34e5>\", line 253, in <module>\n    Dense(4, activation='softmax')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/models.py\", line 408, in __init__\n    self.add(layer)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/models.py\", line 464, in add\n    layer(x)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/layers/wrappers.py\", line 199, in call\n    y = self.layer.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3195, in conv2d\n    data_format=tf_data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 751, in convolution\n    return op(input, filter)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 835, in __call__\n    return self.conv_op(inp, filter)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 499, in __call__\n    return self.call(inp, filter)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 187, in __call__\n    name=self.name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 631, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[480,254,254,16]\n\t [[Node: time_distributed_172/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_172/Reshape, time_distributed_172/kernel/read)]]\n\t [[Node: metrics_16/acc/Mean/_3271 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2576_metrics_16/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6d56bcdf34e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m#h = model.fit(X_PATCH_batch, y_batch, batch_size=train_IMG_batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_PATCH_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mtrain_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_res\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m#print r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1047\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[480,254,254,16]\n\t [[Node: time_distributed_172/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_172/Reshape, time_distributed_172/kernel/read)]]\n\t [[Node: metrics_16/acc/Mean/_3271 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2576_metrics_16/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op u'time_distributed_172/convolution', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-6d56bcdf34e5>\", line 253, in <module>\n    Dense(4, activation='softmax')\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/models.py\", line 408, in __init__\n    self.add(layer)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/models.py\", line 464, in add\n    layer(x)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/layers/wrappers.py\", line 199, in call\n    y = self.layer.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3195, in conv2d\n    data_format=tf_data_format)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 751, in convolution\n    return op(input, filter)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 835, in __call__\n    return self.conv_op(inp, filter)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 499, in __call__\n    return self.call(inp, filter)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 187, in __call__\n    name=self.name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 631, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[480,254,254,16]\n\t [[Node: time_distributed_172/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_172/Reshape, time_distributed_172/kernel/read)]]\n\t [[Node: metrics_16/acc/Mean/_3271 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2576_metrics_16/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# Patch-based Classification of Breast Cancer Histology Images using CNNs\n",
    "# LE48: MiniProject\n",
    "# Jan Ondras (jo356), Trinity College\n",
    "# 2017/2018\n",
    "##################################################################################################\n",
    "##################################################################################\n",
    "# RCNN version, train\n",
    "##################################################################################\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from time import time\n",
    "from math import ceil\n",
    "# from PIL import Image\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, Input, TimeDistributed, LSTM\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "seed = 7 #7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# seeds = [7, 34, 888, 456456, 33, 16947, 7764, 3396, 20500, 3009467] # 10 random seeds = 10 rand restarts\n",
    "\n",
    "# for seed_index, seed in enumerate(seeds): # 10 random restarts\n",
    "#     np.random.seed(seed)\n",
    "#     if seed == 7 or seed == 34:        # TO DELETE\n",
    "#         continue\n",
    "######################################################################################\n",
    "# TO SET\n",
    "######################################################################################\n",
    "overwrite = True\n",
    "# overwrite = False\n",
    "\n",
    "target_size = (256,256)\n",
    "model_ID = 'x00' # + str(seed_index)\n",
    "patch_width = 256                     # 256, 512, 1024\n",
    "patch_height = patch_width\n",
    "patch_stride = patch_width                  \n",
    "model_type = 'RCNN_' + str(patch_width) + '_' + str(patch_stride) + '_' + model_ID\n",
    "\n",
    "RGB_means = [\n",
    "    180.375933345 , 148.276127037 , 174.592562423\n",
    "] \n",
    "# AWS: 180.375933345 , 148.276127037 , 174.592562423\n",
    "# HPC: 180.7795009644826 , 148.3537065177495 , 174.59451631969864\n",
    "######################################################################################\n",
    "# Create model directory for model checkpointing\n",
    "if overwrite == False:\n",
    "    if os.path.exists('./../Model/' + model_type):\n",
    "        raise NameError('Model directory ' + './../Model/' + model_type + ' already exists!')\n",
    "    else:\n",
    "        os.mkdir('./../Model/' + model_type)\n",
    "else:\n",
    "    if not os.path.exists('./../Model/' + model_type):\n",
    "        os.mkdir('./../Model/' + model_type)\n",
    "model_checkpoint_path_prefix = './../Model/' + model_type + '/' + model_type + '_'\n",
    "print model_type\n",
    "\n",
    "# History file for checkpointing loss, acc and time per epoch\n",
    "history_filename = './../History/history_' + model_type + '.txt'\n",
    "if overwrite == False and os.path.exists(history_filename):\n",
    "        raise NameError('History file ' + history_filename + ' already exists!')\n",
    "with open(history_filename, 'w') as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "classes = ['Normal', 'Benign', 'InSitu', 'Invasive'] # correspond to labels 0,1,2,3 in this order\n",
    "\n",
    "# Image size (assume same for all images)\n",
    "img_width =  2048\n",
    "img_height = 1536\n",
    "img_size = (img_height, img_width)\n",
    "pix_scale = 0.42 # micrometers\n",
    "\n",
    "# Number of examples (whole images) per class before data augmentation\n",
    "N_imgs_per_class = 100\n",
    "train_valid_split = 0.75\n",
    "N_train_samples = 300           # in terms of #images\n",
    "N_validation_samples = 100\n",
    "\n",
    "N_patches_x = (((img_width - patch_width) / patch_stride) + 1)\n",
    "N_patches_y = (((img_height - patch_height) / patch_stride) + 1)\n",
    "N_patches_per_img = N_patches_x * N_patches_y\n",
    "print \"Number of patches per image: \", N_patches_per_img\n",
    "\n",
    "path_prefix = './../Dataset/ICIAR2018_BACH_Challenge/Photos_SN_split'\n",
    "train_data_dir = path_prefix + '/train'\n",
    "validation_data_dir = path_prefix + '/validation'\n",
    "\n",
    "epochs = 1000\n",
    "# Batch sizes in terms of number of images;\n",
    "train_IMG_batch_size = 5\n",
    "evaluate_IMG_batch_size = 5\n",
    "\n",
    "print \"Batch size: \", train_IMG_batch_size\n",
    "print \"Evaluate batch size: \", evaluate_IMG_batch_size\n",
    "\n",
    "# Input into model - these are resized patches 256x256; as timeseries of N_patches_per_img\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, N_patches_per_img, target_size[0], target_size[1])\n",
    "    print \"Channels first\"\n",
    "else:\n",
    "    input_shape = (N_patches_per_img, target_size[0], target_size[1], 3) # this one\n",
    "    print \"Channels last\"\n",
    "\n",
    "def substract_mean(img):\n",
    "    # RGB\n",
    "    img[:,:,0] -= RGB_means[0]\n",
    "    img[:,:,1] -= RGB_means[1]\n",
    "    img[:,:,2] -= RGB_means[2]    \n",
    "    return img\n",
    "\n",
    "# def img_to_patches(img):\n",
    "#     '''\n",
    "#     Input = (1536, 2048, 3)\n",
    "#     Output = (N_patches_per_img, 256, 256, 3)\n",
    "#     '''\n",
    "#     patches = np.zeros( (N_patches_per_img, target_size[0], target_size[1], 3) )\n",
    "#     patches = []\n",
    "#     patch_ID = 0\n",
    "#     for i in range(N_patches_x):\n",
    "#         for j in range(N_patches_y):\n",
    "#             p = img[j*patch_stride:j*patch_stride + patch_height, i*patch_stride:i*patch_stride + patch_width, :]\n",
    "#             # Resize to target_size\n",
    "#             patches[patchID] = p\n",
    "#             #patches.append( img[j*patch_stride:j*patch_stride + patch_height, i*patch_stride:i*patch_stride + patch_width, :] )\n",
    "            \n",
    "#             patch_ID += 1\n",
    "#     if patch_ID != N_patches_per_img:\n",
    "#         raise ValueError(\"Incorrect number of patches per image!\")\n",
    "        \n",
    "#     return patches\n",
    "#     #return np.array(patches)\n",
    "    \n",
    "def N_imgs_to_patches(imgs):\n",
    "    '''\n",
    "    Input = (train_IMG_batch_size, 1536, 2048, 3)\n",
    "    Output = (train_IMG_batch_size, N_patches_per_img, 256, 256, 3)\n",
    "    '''\n",
    "    patches = np.zeros( (train_IMG_batch_size, N_patches_per_img, target_size[0], target_size[1], 3) )\n",
    "    # Iterate over images\n",
    "    for imID in range(imgs.shape[0]):\n",
    "    \n",
    "        # Iterate over patches\n",
    "        patchID = 0\n",
    "        for i in range(N_patches_x):\n",
    "            for j in range(N_patches_y):\n",
    "                p = imgs[imID, j*patch_stride:j*patch_stride + patch_height, i*patch_stride:i*patch_stride + patch_width, :]\n",
    "                #print p.shape\n",
    "                # Resize to target_size TODOOOOOOOOO\n",
    "                #Image.fromarray(p).resize(target_size).getdata()\n",
    "                p = cv2.resize(p, target_size, interpolation=cv2.INTER_NEAREST)\n",
    "                patches[imID, patchID] = p\n",
    "                #patches.append( img[j*patch_stride:j*patch_stride + patch_height, i*patch_stride:i*patch_stride + patch_width, :] )\n",
    "\n",
    "                patchID += 1\n",
    "        if patchID != N_patches_per_img:\n",
    "            raise ValueError(\"Incorrect number of patches per image!\")\n",
    "\n",
    "    return patches\n",
    "    #return np.array(patches)\n",
    "\n",
    "# N_imgs_to_patches = np.vectorize(img_to_patches)\n",
    "\n",
    "######################################################################################\n",
    "# SETUP MODEL\n",
    "######################################################################################\n",
    "\n",
    "# model = Sequential([ # ORIGINAL\n",
    "#     TimeDistributed(Conv2D(16, (3, 3), strides=(1, 1), padding='same', activation='relu'), input_shape=input_shape), \n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')), \n",
    "#     TimeDistributed(Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "    \n",
    "#     TimeDistributed(Flatten()), #------------------------------ has dimensionality 32\n",
    "#     LSTM(32, return_sequences=False),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(4, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # From VGG\n",
    "# model = Sequential([ # ORIGINAL\n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block1_conv1'), input_shape=input_shape), \n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block1_conv2')), \n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')),\n",
    "    \n",
    "#     TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same', name='block2_conv1')),\n",
    "#     TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same', name='block2_conv2')),\n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')),\n",
    "    \n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block3_conv1')),\n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')),\n",
    "    \n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block4_conv1')),\n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')),\n",
    "    \n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv1')),\n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')),\n",
    "    \n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv1')),\n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block6_pool')),\n",
    "    \n",
    "#     TimeDistributed(Conv2D(16, (3, 3), activation='relu', padding='same', name='block5_conv1')),\n",
    "#     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block6_pool')),\n",
    "    \n",
    "# #     TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')),\n",
    "# #     TimeDistributed(Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')),\n",
    "# #     TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')),\n",
    "    \n",
    "#     TimeDistributed(Flatten()), #------------------------------ has dimensionality 32\n",
    "#     LSTM(64, return_sequences=False),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(4, activation='softmax')\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "# # Experiemnetnst    0,1,2\n",
    "# model = Sequential([ # ORIGINAL\n",
    "#     TimeDistributed(Conv2D(16, (3, 3), strides=(1, 1), padding='valid', activation='relu'), input_shape=input_shape), \n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=None, padding='valid')), \n",
    "#     TimeDistributed(Conv2D(32, (3, 3), strides=(1, 1), padding='valid', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(32, (3, 3), strides=(1, 1), padding='valid', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=None, padding='valid')),\n",
    "    \n",
    "#     TimeDistributed(Flatten()), #------------------------------ has dimensionality 32\n",
    "#     LSTM(64, return_sequences=False),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dense(4, activation='softmax')\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential([\n",
    "#     TimeDistributed(Conv2D(16, (3, 3), strides=(1, 1), padding='valid', activation='relu'), input_shape=input_shape), \n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=None, padding='valid')), \n",
    "#     TimeDistributed(Conv2D(32, (3, 3), strides=(1, 1), padding='valid', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=None, padding='valid')),\n",
    "#     TimeDistributed(Conv2D(32, (3, 3), strides=(1, 1), padding='valid', activation='relu')),\n",
    "#     TimeDistributed(MaxPooling2D(pool_size=(3, 3), strides=None, padding='valid')),\n",
    "    \n",
    "#     TimeDistributed(Flatten()), #------------------------------ has dimensionality 32\n",
    "#     LSTM(256, dropout=0.5, return_sequences=False),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(4, activation='softmax')\n",
    "# ])\n",
    "print (model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(), # 'adadelta' rmsprop adam\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train and validation generators load whole images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=substract_mean, \n",
    "    rotation_range=180, \n",
    "    horizontal_flip=True, \n",
    "    fill_mode='reflect' # reflect / wrap / nearest\n",
    ") \n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=substract_mean\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size= img_size,\n",
    "    batch_size= train_IMG_batch_size,\n",
    "    class_mode='categorical',\n",
    "    classes=classes)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size= img_size,\n",
    "    batch_size= evaluate_IMG_batch_size,\n",
    "    class_mode='categorical',\n",
    "    classes=classes)\n",
    "\n",
    "######################################################################################\n",
    "# TRAINING & VALIDATION\n",
    "######################################################################################\n",
    "\n",
    "max_val_acc = 0.\n",
    "epochs_wo_improvement = 0\n",
    "patience = 10\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    print \"Epoch {:03d}\".format(e)\n",
    "    st = time()\n",
    "    \n",
    "    train_res = np.zeros(2)\n",
    "    N_batches = 0.\n",
    "    # Get batch of training images\n",
    "    for X_IMG_batch, y_batch in train_generator:\n",
    "        N_batches += 1.\n",
    "        \n",
    "        #print N_batches, X_IMG_batch.shape, y_batch.shape\n",
    "    \n",
    "        # Generate patches for each image\n",
    "        #X_PATCH_batch = N_imgs_to_patches(X_IMG_batch) # this now has shape (train_IMG_batch_size, N_patches_per_img, 256, 256, 3)\n",
    "\n",
    "        X_PATCH_batch = N_imgs_to_patches(X_IMG_batch)  #np.array( map(img_to_patches, X_IMG_batch) )\n",
    "        \n",
    "        #h = model.fit(X_PATCH_batch, y_batch, batch_size=train_IMG_batch_size)\n",
    "        r = model.train_on_batch(X_PATCH_batch, y_batch)\n",
    "        train_res = train_res + r\n",
    "        #print r\n",
    "        \n",
    "        #print h\n",
    "        \n",
    "        # Stop generator if all training examples were seen in this epoch\n",
    "        if N_batches * train_IMG_batch_size >= N_train_samples:\n",
    "            break\n",
    "    train_res = train_res / N_batches\n",
    "    print \"\\tTrain:\", train_res\n",
    "    #print \"#train batches =\", N_batches\n",
    "    \n",
    "    N_batches = 0.\n",
    "    val_res = np.zeros(2)\n",
    "    # Get batch of validation images\n",
    "    for X_IMG_batch, y_batch in validation_generator:\n",
    "        N_batches += 1.\n",
    "\n",
    "        # Generate patches for each image\n",
    "        X_PATCH_batch = N_imgs_to_patches(X_IMG_batch)  #np.array( map(img_to_patches, X_IMG_batch) )\n",
    "\n",
    "        #r = model.evaluate(X_PATCH_batch, y_batch, batch_size=evaluate_IMG_batch_size)\n",
    "        r = model.test_on_batch(X_PATCH_batch, y_batch)\n",
    "        val_res = val_res + r\n",
    "\n",
    "        # Stop generator if all training examples were seen in this epoch\n",
    "        if N_batches * evaluate_IMG_batch_size >= N_validation_samples:\n",
    "            break\n",
    "    val_res = val_res / N_batches\n",
    "    #print \"#val batches =\", N_batches\n",
    "\n",
    "    tpe = time()-st\n",
    "    print \"\\tValid:\", val_res\n",
    "    print \"\\tTime:\", tpe\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save model and results\n",
    "    model.save(model_checkpoint_path_prefix + '{:03d}'.format(e)+'.hdf5')\n",
    "    \n",
    "    with open(history_filename, 'a') as f:\n",
    "        f.write( str(train_res[0]) + \",\" + str(train_res[1]) + \",\" \n",
    "               + str(val_res[0]) + \",\" + str(val_res[1]) + \",\" + str(tpe) + \"\\n\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_res[1] < max_val_acc:\n",
    "        epochs_wo_improvement += 1\n",
    "    else:\n",
    "        epochs_wo_improvement = 0\n",
    "        max_val_acc = val_res[1]\n",
    "    \n",
    "    if epochs_wo_improvement >= patience:\n",
    "        print \"Early stopping ... patience =\", patience\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# OLD, not used\n",
    "\n",
    "##############################################################################\n",
    "# Plot training and validation loss and accuracy curves\n",
    "# Plot training times per epoch\n",
    "##############################################################################\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x = range(1, len(model_hist.times)+1)\n",
    "\n",
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(x, model_hist.train_loss,'ro-')#,linewidth=2.0)\n",
    "plt.plot(x, model_hist.val_loss,'bo-')#,linewidth=2.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'])#,fontsize=18)\n",
    "plt.xticks(x, x)\n",
    "plt.xlabel('Epoch')#,fontsize=16)\n",
    "plt.ylabel('Loss')#,fontsize=16)\n",
    "# plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(x, model_hist.train_acc,'ro-')#,linewidth=2.0)\n",
    "plt.plot(x, model_hist.val_acc,'bo-')#,linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])#,fontsize=18)\n",
    "plt.xticks(x, x)\n",
    "plt.xlabel('Epoch')#,fontsize=16)\n",
    "plt.ylabel('Accuracy')#,fontsize=16)\n",
    "# plt.title('Accuracy Curves',fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Training time\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(x, np.array(model_hist.times)/60., 'o-')\n",
    "plt.xticks(x, x)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training time per epoch (min)')\n",
    "plt.legend(['Mean = '+str(np.mean(model_hist.times)/60.)+\" min\"])\n",
    "plt.show()\n",
    "\n",
    "# Oly if whole training completes ...\n",
    "# # Loss Curves\n",
    "# plt.figure(figsize=[8,6])\n",
    "# plt.plot(history.history['loss'],'r',linewidth=2.0)\n",
    "# plt.plot(history.history['val_loss'],'b',linewidth=2.0)\n",
    "# plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "# plt.xlabel('Epochs ',fontsize=16)\n",
    "# plt.ylabel('Loss',fontsize=16)\n",
    "# plt.title('Loss Curves',fontsize=16)\n",
    "# # Accuracy Curves\n",
    "# plt.figure(figsize=[8,6])\n",
    "# plt.plot(history.history['acc'],'r',linewidth=2.0)\n",
    "# plt.plot(history.history['val_acc'],'b',linewidth=2.0)\n",
    "# plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "# plt.xlabel('Epochs ',fontsize=16)\n",
    "# plt.ylabel('Accuracy',fontsize=16)\n",
    "# plt.title('Accuracy Curves',fontsize=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # just to check\n",
    "    # print train_generator.class_indices, train_generator.classes\n",
    "    # print validation_generator.class_indices, validation_generator.classes\n",
    "\n",
    "    # early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    # model.fit(x, y, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # Reduce learning rate when val_loss stopped improving\n",
    "    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "    #                               patience=5, min_lr=0.001)\n",
    "    # model.fit(X_train, Y_train, callbacks=[reduce_lr])\n",
    "    # or: keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "# Custom generator\n",
    "# https://www.kaggle.com/sinkie/keras-data-augmentation-with-multiple-inputs\n",
    "# def generate_arrays_from_file(path):\n",
    "#     while 1:\n",
    "#         f = open(path)\n",
    "#         for line in f:\n",
    "#             # create numpy arrays of input data\n",
    "#             # and labels, from each line in the file\n",
    "#             x1, x2, y = process_line(line)\n",
    "#             yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
    "#         f.close()\n",
    "\n",
    "# model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
    "#                     steps_per_epoch=10000, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
